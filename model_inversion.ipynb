{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxoOPWrWAeES"
      },
      "source": [
        "# Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy torchvision matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la6fLXzJAfzt",
        "outputId": "6980c844-4458-4a8e-db61-65657655f050"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "izraVMLZAeET"
      },
      "outputs": [],
      "source": [
        "# Required Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "from scipy import linalg\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from torchvision.models import resnet18\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V5LIW5NbAeEU"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq0wUlJUAeEU"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cV8KkDELAeEU",
        "outputId": "ba1bcdd7-55eb-4fc1-94b3-e4f6f9ea298e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:09<00:00, 18.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n"
          ]
        }
      ],
      "source": [
        "# Data Transforms for CIFAR-10\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(128),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 Dataset\n",
        "public_data = datasets.CIFAR10(root='data', train=True, transform=transform, download=True)\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 64\n",
        "public_loader = DataLoader(public_data, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APBzvwEAAeEV"
      },
      "source": [
        "# Model Architechture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIMEnD0dAeEV"
      },
      "source": [
        "### Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Oo4n4WAjAeEV"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim=100, img_channels=3, feature_g=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(z_dim, feature_g * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(feature_g * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(feature_g * 8, feature_g * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_g * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(feature_g * 4, feature_g * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_g * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(feature_g * 2, img_channels, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWTn0bnIAeEV"
      },
      "source": [
        "### Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "y_5j5Q-XAeEV"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_channels=3, feature_d=64, num_classes=11):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(img_channels, feature_d, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(feature_d, feature_d * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_d * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(feature_d * 2, feature_d * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_d * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(feature_d * 4, num_classes, 4, 1, 0, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).view(x.size(0), -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykBqUAhhAeEV"
      },
      "source": [
        "# Training Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAsybMd_AeEV"
      },
      "source": [
        "### Inversion-Specific GAN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tm3SztgfAeEW"
      },
      "outputs": [],
      "source": [
        "def train_inversion_gan(generator, discriminator, target_model, public_loader, num_epochs=50, lr=0.0002):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    optimizer_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (images, _) in enumerate(public_loader):\n",
        "            images = images.to(device)\n",
        "            batch_size = images.size(0)\n",
        "            noise = torch.randn(batch_size, 100, 1, 1, device=device)\n",
        "\n",
        "            # Labels from target model (soft labels)\n",
        "            soft_labels = target_model(images).detach()\n",
        "\n",
        "            # Train Discriminator\n",
        "            real_output = discriminator(images)\n",
        "            fake_images = generator(noise)\n",
        "            fake_output = discriminator(fake_images.detach())\n",
        "\n",
        "            d_loss_real = criterion(real_output, torch.argmax(soft_labels, dim=1))\n",
        "            d_loss_fake = criterion(fake_output, torch.full((batch_size,), 10, dtype=torch.long, device=device))\n",
        "            d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "            optimizer_d.zero_grad()\n",
        "            d_loss.backward()\n",
        "            optimizer_d.step()\n",
        "\n",
        "            # Train Generator\n",
        "            fake_output = discriminator(fake_images)\n",
        "            g_loss = criterion(fake_output, torch.argmax(soft_labels, dim=1))\n",
        "\n",
        "            optimizer_g.zero_grad()\n",
        "            g_loss.backward()\n",
        "            optimizer_g.step()\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Epoch [{epoch}/{num_epochs}], Step [{i}/{len(public_loader)}], D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n",
        "\n",
        "        # Save generated samples for monitoring\n",
        "        save_image(fake_images, f'samples/epoch_{epoch}.png', normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2hCB_-rAeEW"
      },
      "source": [
        "# Distributional Recovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Rn2Wm8G4AeEW"
      },
      "outputs": [],
      "source": [
        "class DistributionalRecovery:\n",
        "    def __init__(self, generator, discriminator, target_model, num_classes, device):\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.target_model = target_model\n",
        "        self.num_classes = num_classes\n",
        "        self.device = device\n",
        "        self.mu = nn.Parameter(torch.zeros(1, 100, 1, 1, device=device), requires_grad=True)\n",
        "        self.sigma = nn.Parameter(torch.ones(1, 100, 1, 1, device=device), requires_grad=True)\n",
        "        self.optimizer = optim.Adam([self.mu, self.sigma], lr=0.01)\n",
        "\n",
        "    def sample_latent(self, num_samples):\n",
        "        epsilon = torch.randn(num_samples, 100, 1, 1, device=self.device)\n",
        "        return self.mu + self.sigma * epsilon\n",
        "\n",
        "    def distributional_loss(self, target_label, lambda_id=100):\n",
        "        z = self.sample_latent(64)\n",
        "        generated_images = self.generator(z)\n",
        "\n",
        "        # Prior Loss: Real vs Fake\n",
        "        realness = self.discriminator(generated_images)\n",
        "        Lprior = -torch.mean(torch.log(torch.sigmoid(realness[:, -1])))\n",
        "\n",
        "        # Identity Loss: Classification Confidence under Target Model\n",
        "        target_confidence = self.target_model(generated_images)\n",
        "        Lid = -torch.mean(torch.log(target_confidence[:, target_label]))\n",
        "\n",
        "        return Lprior + lambda_id * Lid\n",
        "\n",
        "    def update_distribution(self, target_label, num_steps=1500):\n",
        "        for step in range(num_steps):\n",
        "            loss = self.distributional_loss(target_label)\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if step % 100 == 0:\n",
        "                print(f\"Step [{step}/{num_steps}], Loss: {loss.item()}\")\n",
        "\n",
        "    def generate_samples(self, num_samples=64):\n",
        "        z = self.sample_latent(num_samples)\n",
        "        return self.generator(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duLMHGBOAeEW"
      },
      "source": [
        "# Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMrTpYkAAeEW"
      },
      "source": [
        "### Attack Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pA1uiWucAeEW"
      },
      "outputs": [],
      "source": [
        "def calculate_attack_accuracy(generator, evaluation_classifier, target_label, num_samples=100):\n",
        "    evaluation_classifier.eval()\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Generate Samples for the Target Label\n",
        "        z = torch.randn(num_samples, 100, 1, 1, device=device)\n",
        "        generated_images = generator(z)\n",
        "\n",
        "        # Get Predictions from the Evaluation Classifier\n",
        "        outputs = evaluation_classifier(generated_images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Calculate Accuracy for the Target Label\n",
        "        correct += (predicted == target_label).sum().item()\n",
        "\n",
        "    attack_accuracy = correct / num_samples\n",
        "    return attack_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ95AxL9AeEW"
      },
      "source": [
        "### K-Nearest Neighbor Distance (KNN Dist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lYbW6UpAAeEW"
      },
      "outputs": [],
      "source": [
        "def extract_features(images, model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        features = model(images)\n",
        "    return features.cpu().numpy()\n",
        "\n",
        "def calculate_knn_distance(generator, evaluation_classifier, real_loader, target_label, num_samples=100):\n",
        "    evaluation_classifier.eval()\n",
        "\n",
        "    # Step 1: Get Features for Real Images of the Target Label\n",
        "    real_features = []\n",
        "    for images, labels in real_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Filter real images of the target label\n",
        "        target_images = images[labels == target_label]\n",
        "        if len(target_images) > 0:\n",
        "            real_features.append(extract_features(target_images, evaluation_classifier))\n",
        "\n",
        "    real_features = np.vstack(real_features)\n",
        "\n",
        "    # Step 2: Get Features for Generated Images\n",
        "    z = torch.randn(num_samples, 100, 1, 1, device=device)\n",
        "    generated_images = generator(z)\n",
        "    fake_features = extract_features(generated_images, evaluation_classifier)\n",
        "\n",
        "    # Step 3: Calculate K-Nearest Neighbor Distance\n",
        "    knn = NearestNeighbors(n_neighbors=1)\n",
        "    knn.fit(real_features)\n",
        "    distances, _ = knn.kneighbors(fake_features)\n",
        "\n",
        "    knn_dist = np.mean(distances)\n",
        "    return knn_dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0l0-lm_nAeEW"
      },
      "outputs": [],
      "source": [
        "# Helper Function: Calculate Feature Means and Covariances\n",
        "def calculate_statistics(features):\n",
        "    mu = np.mean(features, axis=0)\n",
        "    sigma = np.cov(features, rowvar=False)\n",
        "    return mu, sigma\n",
        "\n",
        "# FID Score Calculation\n",
        "def calculate_fid(real_features, fake_features):\n",
        "    mu_real, sigma_real = calculate_statistics(real_features)\n",
        "    mu_fake, sigma_fake = calculate_statistics(fake_features)\n",
        "\n",
        "    # Calculate FID\n",
        "    diff = mu_real - mu_fake\n",
        "    covmean = linalg.sqrtm(sigma_real.dot(sigma_fake))\n",
        "\n",
        "    # Check for imaginary component from sqrtm\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "\n",
        "    fid = np.sum(diff**2) + np.trace(sigma_real + sigma_fake - 2*covmean)\n",
        "    return fid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the samples directory exists\n",
        "os.makedirs('samples', exist_ok=True)"
      ],
      "metadata": {
        "id": "YIKr3ow5CrmU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ3fnp-QAeEX"
      },
      "source": [
        "# Main Training Loop and Evaluation Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks-I577OAeEX",
        "outputId": "48d6b1f0-3b6d-4a31-859b-e5aefecbe72a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/50], Step [0/782], D Loss: 9.899887084960938, G Loss: 2.918546199798584\n",
            "Epoch [0/50], Step [100/782], D Loss: 2.5068628787994385, G Loss: 7.250673770904541\n",
            "Epoch [0/50], Step [200/782], D Loss: 2.383089542388916, G Loss: 7.997137069702148\n",
            "Epoch [0/50], Step [300/782], D Loss: 2.392315149307251, G Loss: 8.797613143920898\n",
            "Epoch [0/50], Step [400/782], D Loss: 2.4219870567321777, G Loss: 9.412561416625977\n",
            "Epoch [0/50], Step [500/782], D Loss: 2.424818515777588, G Loss: 9.975582122802734\n",
            "Epoch [0/50], Step [600/782], D Loss: 2.426813840866089, G Loss: 9.196457862854004\n",
            "Epoch [0/50], Step [700/782], D Loss: 2.450277805328369, G Loss: 9.454392433166504\n",
            "Epoch [1/50], Step [0/782], D Loss: 2.4408950805664062, G Loss: 11.310224533081055\n",
            "Epoch [1/50], Step [100/782], D Loss: 2.418503522872925, G Loss: 12.271249771118164\n",
            "Epoch [1/50], Step [200/782], D Loss: 2.403623104095459, G Loss: 11.711448669433594\n",
            "Epoch [1/50], Step [300/782], D Loss: 2.3976011276245117, G Loss: 11.001505851745605\n",
            "Epoch [1/50], Step [400/782], D Loss: 2.4362337589263916, G Loss: 11.729266166687012\n",
            "Epoch [1/50], Step [500/782], D Loss: 2.4439618587493896, G Loss: 11.658967971801758\n",
            "Epoch [1/50], Step [600/782], D Loss: 2.4141225814819336, G Loss: 11.33814811706543\n",
            "Epoch [1/50], Step [700/782], D Loss: 2.3898191452026367, G Loss: 11.309203147888184\n",
            "Epoch [2/50], Step [0/782], D Loss: 2.40521502494812, G Loss: 11.381547927856445\n",
            "Epoch [2/50], Step [100/782], D Loss: 2.4300854206085205, G Loss: 11.239605903625488\n",
            "Epoch [2/50], Step [200/782], D Loss: 2.3507652282714844, G Loss: 11.44012451171875\n",
            "Epoch [2/50], Step [300/782], D Loss: 2.422138214111328, G Loss: 11.712504386901855\n",
            "Epoch [2/50], Step [400/782], D Loss: 2.3840348720550537, G Loss: 11.512887001037598\n",
            "Epoch [2/50], Step [500/782], D Loss: 2.3441946506500244, G Loss: 13.431654930114746\n",
            "Epoch [2/50], Step [600/782], D Loss: 2.362337827682495, G Loss: 14.664539337158203\n",
            "Epoch [2/50], Step [700/782], D Loss: 2.391010046005249, G Loss: 13.742745399475098\n",
            "Epoch [3/50], Step [0/782], D Loss: 2.437365770339966, G Loss: 14.562811851501465\n",
            "Epoch [3/50], Step [100/782], D Loss: 2.360103130340576, G Loss: 14.349842071533203\n",
            "Epoch [3/50], Step [200/782], D Loss: 2.388549566268921, G Loss: 12.732769012451172\n",
            "Epoch [3/50], Step [300/782], D Loss: 2.4769551753997803, G Loss: 12.278767585754395\n",
            "Epoch [3/50], Step [400/782], D Loss: 2.4289252758026123, G Loss: 11.889519691467285\n",
            "Epoch [3/50], Step [500/782], D Loss: 2.39990234375, G Loss: 12.177199363708496\n",
            "Epoch [3/50], Step [600/782], D Loss: 2.3658342361450195, G Loss: 13.420616149902344\n",
            "Epoch [3/50], Step [700/782], D Loss: 2.3993911743164062, G Loss: 14.71245002746582\n",
            "Epoch [4/50], Step [0/782], D Loss: 2.432804822921753, G Loss: 15.016528129577637\n",
            "Epoch [4/50], Step [100/782], D Loss: 2.4183847904205322, G Loss: 15.522804260253906\n",
            "Epoch [4/50], Step [200/782], D Loss: 2.402991771697998, G Loss: 15.600499153137207\n",
            "Epoch [4/50], Step [300/782], D Loss: 2.4249181747436523, G Loss: 15.762231826782227\n",
            "Epoch [4/50], Step [400/782], D Loss: 2.3622117042541504, G Loss: 14.40120792388916\n",
            "Epoch [4/50], Step [500/782], D Loss: 2.38485050201416, G Loss: 14.606145858764648\n",
            "Epoch [4/50], Step [600/782], D Loss: 2.430891275405884, G Loss: 15.414443016052246\n",
            "Epoch [4/50], Step [700/782], D Loss: 2.3641488552093506, G Loss: 15.471373558044434\n",
            "Epoch [5/50], Step [0/782], D Loss: 2.4422249794006348, G Loss: 14.333293914794922\n",
            "Epoch [5/50], Step [100/782], D Loss: 2.407289743423462, G Loss: 14.634321212768555\n",
            "Epoch [5/50], Step [200/782], D Loss: 2.387298107147217, G Loss: 14.730097770690918\n",
            "Epoch [5/50], Step [300/782], D Loss: 2.4277946949005127, G Loss: 16.115863800048828\n",
            "Epoch [5/50], Step [400/782], D Loss: 2.401197910308838, G Loss: 17.4011173248291\n",
            "Epoch [5/50], Step [500/782], D Loss: 2.3729004859924316, G Loss: 17.034568786621094\n",
            "Epoch [5/50], Step [600/782], D Loss: 2.4118247032165527, G Loss: 14.903193473815918\n",
            "Epoch [5/50], Step [700/782], D Loss: 2.3590385913848877, G Loss: 14.213635444641113\n",
            "Epoch [6/50], Step [0/782], D Loss: 2.4172449111938477, G Loss: 14.993077278137207\n",
            "Epoch [6/50], Step [100/782], D Loss: 2.439100503921509, G Loss: 16.875621795654297\n",
            "Epoch [6/50], Step [200/782], D Loss: 2.4366979598999023, G Loss: 17.348237991333008\n",
            "Epoch [6/50], Step [300/782], D Loss: 2.4951863288879395, G Loss: 16.537109375\n",
            "Epoch [6/50], Step [400/782], D Loss: 2.52203106880188, G Loss: 17.02910041809082\n",
            "Epoch [6/50], Step [500/782], D Loss: 2.317192554473877, G Loss: 16.298534393310547\n",
            "Epoch [6/50], Step [600/782], D Loss: 2.372917890548706, G Loss: 16.52757453918457\n",
            "Epoch [6/50], Step [700/782], D Loss: 2.3959662914276123, G Loss: 17.122365951538086\n",
            "Epoch [7/50], Step [0/782], D Loss: 2.4197535514831543, G Loss: 17.44440269470215\n",
            "Epoch [7/50], Step [100/782], D Loss: 2.326963424682617, G Loss: 17.548507690429688\n",
            "Epoch [7/50], Step [200/782], D Loss: 2.388474464416504, G Loss: 16.701446533203125\n",
            "Epoch [7/50], Step [300/782], D Loss: 2.3970279693603516, G Loss: 15.129201889038086\n",
            "Epoch [7/50], Step [400/782], D Loss: 2.3781144618988037, G Loss: 16.364246368408203\n",
            "Epoch [7/50], Step [500/782], D Loss: 2.3782811164855957, G Loss: 17.549413681030273\n",
            "Epoch [7/50], Step [600/782], D Loss: 2.4863693714141846, G Loss: 17.607683181762695\n",
            "Epoch [7/50], Step [700/782], D Loss: 2.412881374359131, G Loss: 16.725543975830078\n",
            "Epoch [8/50], Step [0/782], D Loss: 2.3114240169525146, G Loss: 14.004700660705566\n",
            "Epoch [8/50], Step [100/782], D Loss: 2.36755633354187, G Loss: 14.2083740234375\n",
            "Epoch [8/50], Step [200/782], D Loss: 2.3834729194641113, G Loss: 14.820395469665527\n",
            "Epoch [8/50], Step [300/782], D Loss: 2.4068777561187744, G Loss: 14.644636154174805\n",
            "Epoch [8/50], Step [400/782], D Loss: 2.393796682357788, G Loss: 14.403546333312988\n",
            "Epoch [8/50], Step [500/782], D Loss: 2.3408496379852295, G Loss: 14.836320877075195\n",
            "Epoch [8/50], Step [600/782], D Loss: 2.3932173252105713, G Loss: 16.250146865844727\n",
            "Epoch [8/50], Step [700/782], D Loss: 2.3812694549560547, G Loss: 15.378064155578613\n",
            "Epoch [9/50], Step [0/782], D Loss: 2.4707345962524414, G Loss: 15.156452178955078\n",
            "Epoch [9/50], Step [100/782], D Loss: 2.444880962371826, G Loss: 15.29612922668457\n",
            "Epoch [9/50], Step [200/782], D Loss: 2.352998971939087, G Loss: 16.201961517333984\n",
            "Epoch [9/50], Step [300/782], D Loss: 2.383143901824951, G Loss: 15.975642204284668\n",
            "Epoch [9/50], Step [400/782], D Loss: 2.396200656890869, G Loss: 16.141529083251953\n",
            "Epoch [9/50], Step [500/782], D Loss: 2.3958442211151123, G Loss: 15.348709106445312\n",
            "Epoch [9/50], Step [600/782], D Loss: 2.399911403656006, G Loss: 16.59324836730957\n",
            "Epoch [9/50], Step [700/782], D Loss: 2.3682286739349365, G Loss: 15.08643627166748\n",
            "Epoch [10/50], Step [0/782], D Loss: 2.350205659866333, G Loss: 16.59809684753418\n",
            "Epoch [10/50], Step [100/782], D Loss: 2.353713274002075, G Loss: 16.445035934448242\n",
            "Epoch [10/50], Step [200/782], D Loss: 2.3966853618621826, G Loss: 15.786105155944824\n",
            "Epoch [10/50], Step [300/782], D Loss: 2.384099245071411, G Loss: 15.244684219360352\n",
            "Epoch [10/50], Step [400/782], D Loss: 2.332925796508789, G Loss: 16.052616119384766\n",
            "Epoch [10/50], Step [500/782], D Loss: 2.338027238845825, G Loss: 16.136016845703125\n",
            "Epoch [10/50], Step [600/782], D Loss: 2.4084997177124023, G Loss: 15.960491180419922\n",
            "Epoch [10/50], Step [700/782], D Loss: 2.3544459342956543, G Loss: 16.424293518066406\n",
            "Epoch [11/50], Step [0/782], D Loss: 2.422299385070801, G Loss: 15.986967086791992\n",
            "Epoch [11/50], Step [100/782], D Loss: 2.3021390438079834, G Loss: 16.457229614257812\n",
            "Epoch [11/50], Step [200/782], D Loss: 2.352487802505493, G Loss: 15.781546592712402\n",
            "Epoch [11/50], Step [300/782], D Loss: 2.3857309818267822, G Loss: 17.407808303833008\n",
            "Epoch [11/50], Step [400/782], D Loss: 2.3924384117126465, G Loss: 16.933433532714844\n",
            "Epoch [11/50], Step [500/782], D Loss: 2.3983407020568848, G Loss: 16.765947341918945\n",
            "Epoch [11/50], Step [600/782], D Loss: 2.3793351650238037, G Loss: 18.966386795043945\n",
            "Epoch [11/50], Step [700/782], D Loss: 2.346895456314087, G Loss: 16.92282485961914\n",
            "Epoch [12/50], Step [0/782], D Loss: 2.420118570327759, G Loss: 17.416135787963867\n",
            "Epoch [12/50], Step [100/782], D Loss: 2.3392062187194824, G Loss: 18.114662170410156\n",
            "Epoch [12/50], Step [200/782], D Loss: 2.270277500152588, G Loss: 16.84227180480957\n",
            "Epoch [12/50], Step [300/782], D Loss: 2.4387545585632324, G Loss: 17.413070678710938\n",
            "Epoch [12/50], Step [400/782], D Loss: 2.393540382385254, G Loss: 17.01445960998535\n",
            "Epoch [12/50], Step [500/782], D Loss: 2.333958864212036, G Loss: 16.652259826660156\n",
            "Epoch [12/50], Step [600/782], D Loss: 2.383323907852173, G Loss: 17.52164649963379\n",
            "Epoch [12/50], Step [700/782], D Loss: 2.3764736652374268, G Loss: 16.597095489501953\n",
            "Epoch [13/50], Step [0/782], D Loss: 2.328951597213745, G Loss: 16.77867317199707\n",
            "Epoch [13/50], Step [100/782], D Loss: 2.3577466011047363, G Loss: 15.69662857055664\n",
            "Epoch [13/50], Step [200/782], D Loss: 2.3926541805267334, G Loss: 16.774065017700195\n",
            "Epoch [13/50], Step [300/782], D Loss: 2.365079879760742, G Loss: 17.410236358642578\n",
            "Epoch [13/50], Step [400/782], D Loss: 2.3868181705474854, G Loss: 16.787307739257812\n",
            "Epoch [13/50], Step [500/782], D Loss: 2.3432729244232178, G Loss: 18.963909149169922\n",
            "Epoch [13/50], Step [600/782], D Loss: 2.3786213397979736, G Loss: 17.422382354736328\n",
            "Epoch [13/50], Step [700/782], D Loss: 2.4342947006225586, G Loss: 15.611795425415039\n",
            "Epoch [14/50], Step [0/782], D Loss: 2.3529956340789795, G Loss: 17.100215911865234\n",
            "Epoch [14/50], Step [100/782], D Loss: 2.4109559059143066, G Loss: 19.096027374267578\n",
            "Epoch [14/50], Step [200/782], D Loss: 2.3013522624969482, G Loss: 16.419198989868164\n",
            "Epoch [14/50], Step [300/782], D Loss: 2.3400492668151855, G Loss: 17.476539611816406\n",
            "Epoch [14/50], Step [400/782], D Loss: 2.378997325897217, G Loss: 17.5313663482666\n",
            "Epoch [14/50], Step [500/782], D Loss: 2.354797601699829, G Loss: 17.4040470123291\n",
            "Epoch [14/50], Step [600/782], D Loss: 2.3739371299743652, G Loss: 21.653806686401367\n",
            "Epoch [14/50], Step [700/782], D Loss: 2.4214398860931396, G Loss: 17.677600860595703\n",
            "Epoch [15/50], Step [0/782], D Loss: 2.3227522373199463, G Loss: 18.201995849609375\n",
            "Epoch [15/50], Step [100/782], D Loss: 2.2860679626464844, G Loss: 17.84932518005371\n",
            "Epoch [15/50], Step [200/782], D Loss: 2.3315389156341553, G Loss: 19.73220443725586\n",
            "Epoch [15/50], Step [300/782], D Loss: 2.392605781555176, G Loss: 16.816112518310547\n",
            "Epoch [15/50], Step [400/782], D Loss: 2.4102120399475098, G Loss: 21.24812126159668\n",
            "Epoch [15/50], Step [500/782], D Loss: 2.3414902687072754, G Loss: 16.254623413085938\n",
            "Epoch [15/50], Step [600/782], D Loss: 2.273102283477783, G Loss: 16.598421096801758\n",
            "Epoch [15/50], Step [700/782], D Loss: 2.4102067947387695, G Loss: 17.04901885986328\n",
            "Epoch [16/50], Step [0/782], D Loss: 2.2941596508026123, G Loss: 18.403514862060547\n",
            "Epoch [16/50], Step [100/782], D Loss: 2.37333083152771, G Loss: 16.407752990722656\n",
            "Epoch [16/50], Step [200/782], D Loss: 2.354759931564331, G Loss: 16.690710067749023\n",
            "Epoch [16/50], Step [300/782], D Loss: 2.4194693565368652, G Loss: 17.157135009765625\n",
            "Epoch [16/50], Step [400/782], D Loss: 2.2993807792663574, G Loss: 16.495349884033203\n",
            "Epoch [16/50], Step [500/782], D Loss: 2.3244495391845703, G Loss: 19.625347137451172\n",
            "Epoch [16/50], Step [600/782], D Loss: 2.4203426837921143, G Loss: 17.418495178222656\n",
            "Epoch [16/50], Step [700/782], D Loss: 2.341562271118164, G Loss: 19.35630226135254\n",
            "Epoch [17/50], Step [0/782], D Loss: 2.2757043838500977, G Loss: 17.584043502807617\n",
            "Epoch [17/50], Step [100/782], D Loss: 2.2534103393554688, G Loss: 17.604652404785156\n",
            "Epoch [17/50], Step [200/782], D Loss: 2.3603012561798096, G Loss: 17.267568588256836\n",
            "Epoch [17/50], Step [300/782], D Loss: 2.3659422397613525, G Loss: 17.979339599609375\n",
            "Epoch [17/50], Step [400/782], D Loss: 2.36409592628479, G Loss: 18.585052490234375\n",
            "Epoch [17/50], Step [500/782], D Loss: 2.32297420501709, G Loss: 17.791383743286133\n",
            "Epoch [17/50], Step [600/782], D Loss: 2.3408620357513428, G Loss: 17.798141479492188\n",
            "Epoch [17/50], Step [700/782], D Loss: 2.3438329696655273, G Loss: 17.405061721801758\n",
            "Epoch [18/50], Step [0/782], D Loss: 2.483093500137329, G Loss: 19.578227996826172\n",
            "Epoch [18/50], Step [100/782], D Loss: 2.287108898162842, G Loss: 16.294015884399414\n",
            "Epoch [18/50], Step [200/782], D Loss: 2.3011436462402344, G Loss: 18.312118530273438\n",
            "Epoch [18/50], Step [300/782], D Loss: 2.3163273334503174, G Loss: 16.997533798217773\n",
            "Epoch [18/50], Step [400/782], D Loss: 2.346113443374634, G Loss: 17.28873634338379\n",
            "Epoch [18/50], Step [500/782], D Loss: 2.3000972270965576, G Loss: 19.289051055908203\n",
            "Epoch [18/50], Step [600/782], D Loss: 2.3471686840057373, G Loss: 18.064001083374023\n",
            "Epoch [18/50], Step [700/782], D Loss: 2.265061616897583, G Loss: 16.440574645996094\n",
            "Epoch [19/50], Step [0/782], D Loss: 2.330491781234741, G Loss: 16.570932388305664\n",
            "Epoch [19/50], Step [100/782], D Loss: 2.3133554458618164, G Loss: 16.080928802490234\n",
            "Epoch [19/50], Step [200/782], D Loss: 2.4013164043426514, G Loss: 18.15713119506836\n",
            "Epoch [19/50], Step [300/782], D Loss: 2.356309652328491, G Loss: 18.560794830322266\n",
            "Epoch [19/50], Step [400/782], D Loss: 2.3941004276275635, G Loss: 17.355987548828125\n",
            "Epoch [19/50], Step [500/782], D Loss: 2.3021295070648193, G Loss: 17.49189567565918\n",
            "Epoch [19/50], Step [600/782], D Loss: 2.4048566818237305, G Loss: 17.454174041748047\n",
            "Epoch [19/50], Step [700/782], D Loss: 2.3484766483306885, G Loss: 16.682443618774414\n",
            "Epoch [20/50], Step [0/782], D Loss: 2.2749459743499756, G Loss: 18.33730697631836\n",
            "Epoch [20/50], Step [100/782], D Loss: 2.218092918395996, G Loss: 18.578161239624023\n",
            "Epoch [20/50], Step [200/782], D Loss: 2.2669010162353516, G Loss: 17.05512237548828\n",
            "Epoch [20/50], Step [300/782], D Loss: 2.335441827774048, G Loss: 22.03029441833496\n",
            "Epoch [20/50], Step [400/782], D Loss: 2.3464393615722656, G Loss: 17.700090408325195\n",
            "Epoch [20/50], Step [500/782], D Loss: 2.3287034034729004, G Loss: 18.860557556152344\n",
            "Epoch [20/50], Step [600/782], D Loss: 2.3569881916046143, G Loss: 18.136919021606445\n",
            "Epoch [20/50], Step [700/782], D Loss: 2.2426915168762207, G Loss: 17.754411697387695\n",
            "Epoch [21/50], Step [0/782], D Loss: 2.315521001815796, G Loss: 17.944581985473633\n",
            "Epoch [21/50], Step [100/782], D Loss: 2.2720775604248047, G Loss: 16.82130241394043\n",
            "Epoch [21/50], Step [200/782], D Loss: 2.355515241622925, G Loss: 20.65454864501953\n",
            "Epoch [21/50], Step [300/782], D Loss: 2.3634798526763916, G Loss: 19.809913635253906\n",
            "Epoch [21/50], Step [400/782], D Loss: 2.3532087802886963, G Loss: 19.143939971923828\n",
            "Epoch [21/50], Step [500/782], D Loss: 2.3060736656188965, G Loss: 19.606843948364258\n",
            "Epoch [21/50], Step [600/782], D Loss: 2.374011516571045, G Loss: 19.124698638916016\n",
            "Epoch [21/50], Step [700/782], D Loss: 2.246206045150757, G Loss: 29.81876564025879\n",
            "Epoch [22/50], Step [0/782], D Loss: 2.270503520965576, G Loss: 18.20112419128418\n",
            "Epoch [22/50], Step [100/782], D Loss: 2.178874969482422, G Loss: 16.157331466674805\n",
            "Epoch [22/50], Step [200/782], D Loss: 2.2934257984161377, G Loss: 17.417009353637695\n",
            "Epoch [22/50], Step [300/782], D Loss: 2.29215669631958, G Loss: 19.270309448242188\n",
            "Epoch [22/50], Step [400/782], D Loss: 2.3101813793182373, G Loss: 16.750715255737305\n",
            "Epoch [22/50], Step [500/782], D Loss: 2.276890516281128, G Loss: 17.627296447753906\n",
            "Epoch [22/50], Step [600/782], D Loss: 2.287628173828125, G Loss: 16.734420776367188\n",
            "Epoch [22/50], Step [700/782], D Loss: 2.4353601932525635, G Loss: 18.20712661743164\n",
            "Epoch [23/50], Step [0/782], D Loss: 2.2606334686279297, G Loss: 17.784990310668945\n",
            "Epoch [23/50], Step [100/782], D Loss: 2.2120485305786133, G Loss: 20.785646438598633\n",
            "Epoch [23/50], Step [200/782], D Loss: 2.3069088459014893, G Loss: 15.924177169799805\n",
            "Epoch [23/50], Step [300/782], D Loss: 2.321599006652832, G Loss: 28.010547637939453\n",
            "Epoch [23/50], Step [400/782], D Loss: 2.283022880554199, G Loss: 19.3275203704834\n",
            "Epoch [23/50], Step [500/782], D Loss: 2.326503276824951, G Loss: 25.809703826904297\n",
            "Epoch [23/50], Step [600/782], D Loss: 2.291107416152954, G Loss: 17.847217559814453\n",
            "Epoch [23/50], Step [700/782], D Loss: 2.2953977584838867, G Loss: 17.80546760559082\n",
            "Epoch [24/50], Step [0/782], D Loss: 2.3048441410064697, G Loss: 20.940940856933594\n",
            "Epoch [24/50], Step [100/782], D Loss: 2.2223896980285645, G Loss: 16.308439254760742\n",
            "Epoch [24/50], Step [200/782], D Loss: 2.207155466079712, G Loss: 18.72359848022461\n",
            "Epoch [24/50], Step [300/782], D Loss: 2.207509756088257, G Loss: 17.64048957824707\n",
            "Epoch [24/50], Step [400/782], D Loss: 2.2395546436309814, G Loss: 19.300718307495117\n",
            "Epoch [24/50], Step [500/782], D Loss: 2.226384401321411, G Loss: 17.453472137451172\n",
            "Epoch [24/50], Step [600/782], D Loss: 2.3017916679382324, G Loss: 17.373876571655273\n",
            "Epoch [24/50], Step [700/782], D Loss: 2.284357786178589, G Loss: 20.828523635864258\n",
            "Epoch [25/50], Step [0/782], D Loss: 2.239581823348999, G Loss: 17.812881469726562\n",
            "Epoch [25/50], Step [100/782], D Loss: 2.2751622200012207, G Loss: 18.8929443359375\n",
            "Epoch [25/50], Step [200/782], D Loss: 2.3562073707580566, G Loss: 17.59274673461914\n",
            "Epoch [25/50], Step [300/782], D Loss: 2.2553927898406982, G Loss: 21.66058921813965\n",
            "Epoch [25/50], Step [400/782], D Loss: 2.3077144622802734, G Loss: 19.351104736328125\n",
            "Epoch [25/50], Step [500/782], D Loss: 2.294651985168457, G Loss: 17.12032699584961\n",
            "Epoch [25/50], Step [600/782], D Loss: 2.302102565765381, G Loss: 18.95396614074707\n",
            "Epoch [25/50], Step [700/782], D Loss: 2.2825863361358643, G Loss: 18.524778366088867\n",
            "Epoch [26/50], Step [0/782], D Loss: 2.226003885269165, G Loss: 19.091938018798828\n",
            "Epoch [26/50], Step [100/782], D Loss: 2.2183618545532227, G Loss: 18.20525550842285\n",
            "Epoch [26/50], Step [200/782], D Loss: 2.2123069763183594, G Loss: 16.653669357299805\n",
            "Epoch [26/50], Step [300/782], D Loss: 2.221831798553467, G Loss: 20.823638916015625\n",
            "Epoch [26/50], Step [400/782], D Loss: 2.309882402420044, G Loss: 17.50750732421875\n",
            "Epoch [26/50], Step [500/782], D Loss: 2.351513385772705, G Loss: 16.943037033081055\n",
            "Epoch [26/50], Step [600/782], D Loss: 2.4105913639068604, G Loss: 18.73619270324707\n",
            "Epoch [26/50], Step [700/782], D Loss: 2.3416895866394043, G Loss: 46.78032302856445\n",
            "Epoch [27/50], Step [0/782], D Loss: 2.188453435897827, G Loss: 18.820457458496094\n",
            "Epoch [27/50], Step [100/782], D Loss: 2.2661473751068115, G Loss: 18.456830978393555\n",
            "Epoch [27/50], Step [200/782], D Loss: 2.419118642807007, G Loss: 17.628429412841797\n",
            "Epoch [27/50], Step [300/782], D Loss: 2.2348103523254395, G Loss: 18.97068214416504\n",
            "Epoch [27/50], Step [400/782], D Loss: 2.3421642780303955, G Loss: 19.990127563476562\n",
            "Epoch [27/50], Step [500/782], D Loss: 2.3120367527008057, G Loss: 19.876150131225586\n",
            "Epoch [27/50], Step [600/782], D Loss: 2.200619697570801, G Loss: 17.417888641357422\n",
            "Epoch [27/50], Step [700/782], D Loss: 2.261756658554077, G Loss: 25.1560001373291\n",
            "Epoch [28/50], Step [0/782], D Loss: 2.198296546936035, G Loss: 18.160520553588867\n",
            "Epoch [28/50], Step [100/782], D Loss: 2.3027093410491943, G Loss: 25.007694244384766\n",
            "Epoch [28/50], Step [200/782], D Loss: 2.248265266418457, G Loss: 18.9291934967041\n",
            "Epoch [28/50], Step [300/782], D Loss: 2.321587562561035, G Loss: 20.676311492919922\n",
            "Epoch [28/50], Step [400/782], D Loss: 2.3330345153808594, G Loss: 21.768478393554688\n",
            "Epoch [28/50], Step [500/782], D Loss: 2.2102138996124268, G Loss: 20.02167320251465\n",
            "Epoch [28/50], Step [600/782], D Loss: 2.242522954940796, G Loss: 18.87567710876465\n",
            "Epoch [28/50], Step [700/782], D Loss: 2.328796148300171, G Loss: 18.061864852905273\n",
            "Epoch [29/50], Step [0/782], D Loss: 2.1338448524475098, G Loss: 20.313798904418945\n",
            "Epoch [29/50], Step [100/782], D Loss: 2.3319759368896484, G Loss: 18.801925659179688\n",
            "Epoch [29/50], Step [200/782], D Loss: 2.271559953689575, G Loss: 23.740694046020508\n",
            "Epoch [29/50], Step [300/782], D Loss: 2.2100040912628174, G Loss: 35.16205596923828\n",
            "Epoch [29/50], Step [400/782], D Loss: 2.281449794769287, G Loss: 20.92679214477539\n",
            "Epoch [29/50], Step [500/782], D Loss: 2.311237096786499, G Loss: 22.25271987915039\n",
            "Epoch [29/50], Step [600/782], D Loss: 2.132885217666626, G Loss: 24.71923065185547\n",
            "Epoch [29/50], Step [700/782], D Loss: 2.2877345085144043, G Loss: 28.23824691772461\n",
            "Epoch [30/50], Step [0/782], D Loss: 2.2319040298461914, G Loss: 22.874475479125977\n",
            "Epoch [30/50], Step [100/782], D Loss: 2.258817434310913, G Loss: 20.7037410736084\n",
            "Epoch [30/50], Step [200/782], D Loss: 2.196439266204834, G Loss: 19.132404327392578\n",
            "Epoch [30/50], Step [300/782], D Loss: 2.223186731338501, G Loss: 21.532812118530273\n",
            "Epoch [30/50], Step [400/782], D Loss: 2.340104579925537, G Loss: 19.467926025390625\n",
            "Epoch [30/50], Step [500/782], D Loss: 2.2502152919769287, G Loss: 22.860048294067383\n",
            "Epoch [30/50], Step [600/782], D Loss: 2.2070634365081787, G Loss: 20.64862823486328\n",
            "Epoch [30/50], Step [700/782], D Loss: 2.3020052909851074, G Loss: 19.394556045532227\n",
            "Epoch [31/50], Step [0/782], D Loss: 2.23382568359375, G Loss: 21.860401153564453\n",
            "Epoch [31/50], Step [100/782], D Loss: 2.1980669498443604, G Loss: 19.20384979248047\n",
            "Epoch [31/50], Step [200/782], D Loss: 2.125218629837036, G Loss: 19.294103622436523\n",
            "Epoch [31/50], Step [300/782], D Loss: 2.1246299743652344, G Loss: 18.554059982299805\n",
            "Epoch [31/50], Step [400/782], D Loss: 2.220395565032959, G Loss: 22.74733543395996\n",
            "Epoch [31/50], Step [500/782], D Loss: 2.1201231479644775, G Loss: 20.80194854736328\n",
            "Epoch [31/50], Step [600/782], D Loss: 2.2519004344940186, G Loss: 18.226818084716797\n",
            "Epoch [31/50], Step [700/782], D Loss: 2.282090902328491, G Loss: 22.22646713256836\n",
            "Epoch [32/50], Step [0/782], D Loss: 2.11217999458313, G Loss: 18.834701538085938\n",
            "Epoch [32/50], Step [100/782], D Loss: 2.3176114559173584, G Loss: 19.854087829589844\n",
            "Epoch [32/50], Step [200/782], D Loss: 2.1916446685791016, G Loss: 20.61507225036621\n",
            "Epoch [32/50], Step [300/782], D Loss: 2.2738044261932373, G Loss: 18.699993133544922\n",
            "Epoch [32/50], Step [400/782], D Loss: 2.2139105796813965, G Loss: 24.302589416503906\n",
            "Epoch [32/50], Step [500/782], D Loss: 2.191342830657959, G Loss: 20.230087280273438\n",
            "Epoch [32/50], Step [600/782], D Loss: 2.2520127296447754, G Loss: 18.957006454467773\n",
            "Epoch [32/50], Step [700/782], D Loss: 2.196012496948242, G Loss: 19.35586929321289\n",
            "Epoch [33/50], Step [0/782], D Loss: 2.0783557891845703, G Loss: 20.355398178100586\n",
            "Epoch [33/50], Step [100/782], D Loss: 2.1900007724761963, G Loss: 18.94786834716797\n",
            "Epoch [33/50], Step [200/782], D Loss: 2.2634618282318115, G Loss: 19.9136905670166\n",
            "Epoch [33/50], Step [300/782], D Loss: 2.2131857872009277, G Loss: 21.352497100830078\n",
            "Epoch [33/50], Step [400/782], D Loss: 2.135690212249756, G Loss: 21.779760360717773\n",
            "Epoch [33/50], Step [500/782], D Loss: 2.044403314590454, G Loss: 21.92727279663086\n",
            "Epoch [33/50], Step [600/782], D Loss: 2.3296573162078857, G Loss: 19.40323829650879\n",
            "Epoch [33/50], Step [700/782], D Loss: 2.145568370819092, G Loss: 21.810562133789062\n",
            "Epoch [34/50], Step [0/782], D Loss: 2.1177866458892822, G Loss: 30.224609375\n",
            "Epoch [34/50], Step [100/782], D Loss: 2.134706497192383, G Loss: 22.3800106048584\n",
            "Epoch [34/50], Step [200/782], D Loss: 2.2152442932128906, G Loss: 23.933332443237305\n",
            "Epoch [34/50], Step [300/782], D Loss: 2.3032474517822266, G Loss: 21.571033477783203\n",
            "Epoch [34/50], Step [400/782], D Loss: 2.1507627964019775, G Loss: 20.172584533691406\n",
            "Epoch [34/50], Step [500/782], D Loss: 2.3939430713653564, G Loss: 22.577749252319336\n",
            "Epoch [34/50], Step [600/782], D Loss: 2.3092377185821533, G Loss: 18.95382308959961\n",
            "Epoch [34/50], Step [700/782], D Loss: 2.3882229328155518, G Loss: 18.148792266845703\n",
            "Epoch [35/50], Step [0/782], D Loss: 2.138094663619995, G Loss: 25.487092971801758\n",
            "Epoch [35/50], Step [100/782], D Loss: 2.1545584201812744, G Loss: 22.662752151489258\n",
            "Epoch [35/50], Step [200/782], D Loss: 1.9958442449569702, G Loss: 19.849517822265625\n",
            "Epoch [35/50], Step [300/782], D Loss: 2.2911243438720703, G Loss: 25.648706436157227\n",
            "Epoch [35/50], Step [400/782], D Loss: 2.25382137298584, G Loss: 19.421472549438477\n",
            "Epoch [35/50], Step [500/782], D Loss: 2.3157262802124023, G Loss: 22.266475677490234\n",
            "Epoch [35/50], Step [600/782], D Loss: 2.231419086456299, G Loss: 18.909168243408203\n",
            "Epoch [35/50], Step [700/782], D Loss: 2.1637747287750244, G Loss: 18.855880737304688\n",
            "Epoch [36/50], Step [0/782], D Loss: 2.047398805618286, G Loss: 22.17380714416504\n",
            "Epoch [36/50], Step [100/782], D Loss: 2.1214261054992676, G Loss: 23.67658805847168\n",
            "Epoch [36/50], Step [200/782], D Loss: 2.166072368621826, G Loss: 22.481258392333984\n",
            "Epoch [36/50], Step [300/782], D Loss: 2.174600839614868, G Loss: 19.719818115234375\n",
            "Epoch [36/50], Step [400/782], D Loss: 2.3266332149505615, G Loss: 20.287534713745117\n",
            "Epoch [36/50], Step [500/782], D Loss: 2.225005626678467, G Loss: 22.87517738342285\n",
            "Epoch [36/50], Step [600/782], D Loss: 2.1592230796813965, G Loss: 18.542089462280273\n",
            "Epoch [36/50], Step [700/782], D Loss: 2.2868521213531494, G Loss: 19.03141975402832\n",
            "Epoch [37/50], Step [0/782], D Loss: 2.0574545860290527, G Loss: 22.3983154296875\n",
            "Epoch [37/50], Step [100/782], D Loss: 2.3347551822662354, G Loss: 20.23179054260254\n",
            "Epoch [37/50], Step [200/782], D Loss: 2.104478597640991, G Loss: 21.133150100708008\n",
            "Epoch [37/50], Step [300/782], D Loss: 2.135460138320923, G Loss: 23.309200286865234\n",
            "Epoch [37/50], Step [400/782], D Loss: 2.2438607215881348, G Loss: 20.57717514038086\n",
            "Epoch [37/50], Step [500/782], D Loss: 2.0137758255004883, G Loss: 20.36508560180664\n",
            "Epoch [37/50], Step [600/782], D Loss: 2.1512510776519775, G Loss: 18.989835739135742\n",
            "Epoch [37/50], Step [700/782], D Loss: 2.157963275909424, G Loss: 21.947174072265625\n",
            "Epoch [38/50], Step [0/782], D Loss: 1.9588371515274048, G Loss: 25.629396438598633\n",
            "Epoch [38/50], Step [100/782], D Loss: 2.1651411056518555, G Loss: 33.02113723754883\n",
            "Epoch [38/50], Step [200/782], D Loss: 2.277862787246704, G Loss: 22.166385650634766\n",
            "Epoch [38/50], Step [300/782], D Loss: 2.384683609008789, G Loss: 20.067270278930664\n",
            "Epoch [38/50], Step [400/782], D Loss: 2.1677744388580322, G Loss: 19.904544830322266\n",
            "Epoch [38/50], Step [500/782], D Loss: 2.167985439300537, G Loss: 22.1668701171875\n",
            "Epoch [38/50], Step [600/782], D Loss: 2.1177845001220703, G Loss: 21.29688835144043\n",
            "Epoch [38/50], Step [700/782], D Loss: 2.1513500213623047, G Loss: 19.845888137817383\n",
            "Epoch [39/50], Step [0/782], D Loss: 1.917933702468872, G Loss: 23.95608901977539\n",
            "Epoch [39/50], Step [100/782], D Loss: 2.045896291732788, G Loss: 23.487836837768555\n",
            "Epoch [39/50], Step [200/782], D Loss: 1.9396476745605469, G Loss: 23.138267517089844\n",
            "Epoch [39/50], Step [300/782], D Loss: 2.048907518386841, G Loss: 20.88703155517578\n",
            "Epoch [39/50], Step [400/782], D Loss: 1.9708577394485474, G Loss: 20.686689376831055\n",
            "Epoch [39/50], Step [500/782], D Loss: 2.1911873817443848, G Loss: 22.95505714416504\n",
            "Epoch [39/50], Step [600/782], D Loss: 2.2414584159851074, G Loss: 30.333206176757812\n",
            "Epoch [39/50], Step [700/782], D Loss: 2.0531342029571533, G Loss: 25.5701904296875\n",
            "Epoch [40/50], Step [0/782], D Loss: 1.9603017568588257, G Loss: 24.207670211791992\n",
            "Epoch [40/50], Step [100/782], D Loss: 2.146972894668579, G Loss: 21.500572204589844\n",
            "Epoch [40/50], Step [200/782], D Loss: 2.217644691467285, G Loss: 25.429401397705078\n",
            "Epoch [40/50], Step [300/782], D Loss: 1.973119854927063, G Loss: 25.159292221069336\n",
            "Epoch [40/50], Step [400/782], D Loss: 2.013287305831909, G Loss: 22.157804489135742\n",
            "Epoch [40/50], Step [500/782], D Loss: 2.0213139057159424, G Loss: 21.2144718170166\n",
            "Epoch [40/50], Step [600/782], D Loss: 1.9595152139663696, G Loss: 22.72037124633789\n",
            "Epoch [40/50], Step [700/782], D Loss: 2.0565438270568848, G Loss: 26.20905876159668\n",
            "Epoch [41/50], Step [0/782], D Loss: 2.1494505405426025, G Loss: 26.656982421875\n",
            "Epoch [41/50], Step [100/782], D Loss: 2.0176472663879395, G Loss: 21.86136817932129\n",
            "Epoch [41/50], Step [200/782], D Loss: 2.153088092803955, G Loss: 21.222566604614258\n",
            "Epoch [41/50], Step [300/782], D Loss: 2.011155843734741, G Loss: 33.57007598876953\n",
            "Epoch [41/50], Step [400/782], D Loss: 2.117581605911255, G Loss: 26.73565101623535\n",
            "Epoch [41/50], Step [500/782], D Loss: 2.1683743000030518, G Loss: 22.221982955932617\n",
            "Epoch [41/50], Step [600/782], D Loss: 2.0569777488708496, G Loss: 24.856414794921875\n",
            "Epoch [41/50], Step [700/782], D Loss: 2.092015266418457, G Loss: 25.19013786315918\n",
            "Epoch [42/50], Step [0/782], D Loss: 1.9706064462661743, G Loss: 24.473669052124023\n",
            "Epoch [42/50], Step [100/782], D Loss: 2.1345245838165283, G Loss: 31.11509895324707\n",
            "Epoch [42/50], Step [200/782], D Loss: 2.157571315765381, G Loss: 21.21126937866211\n",
            "Epoch [42/50], Step [300/782], D Loss: 2.09466814994812, G Loss: 24.37781524658203\n",
            "Epoch [42/50], Step [400/782], D Loss: 2.0096006393432617, G Loss: 27.608428955078125\n",
            "Epoch [42/50], Step [500/782], D Loss: 2.1876697540283203, G Loss: 26.10399627685547\n",
            "Epoch [42/50], Step [600/782], D Loss: 2.188159942626953, G Loss: 21.32616424560547\n",
            "Epoch [42/50], Step [700/782], D Loss: 1.984442114830017, G Loss: 21.013282775878906\n",
            "Epoch [43/50], Step [0/782], D Loss: 2.0096333026885986, G Loss: 30.119497299194336\n",
            "Epoch [43/50], Step [100/782], D Loss: 2.102018356323242, G Loss: 23.983535766601562\n",
            "Epoch [43/50], Step [200/782], D Loss: 2.0854179859161377, G Loss: 20.176654815673828\n",
            "Epoch [43/50], Step [300/782], D Loss: 2.074584484100342, G Loss: 22.243961334228516\n",
            "Epoch [43/50], Step [400/782], D Loss: 2.1827564239501953, G Loss: 20.409744262695312\n",
            "Epoch [43/50], Step [500/782], D Loss: 2.1358964443206787, G Loss: 21.221540451049805\n",
            "Epoch [43/50], Step [600/782], D Loss: 1.8119975328445435, G Loss: 21.055614471435547\n",
            "Epoch [43/50], Step [700/782], D Loss: 2.0652356147766113, G Loss: 22.10403060913086\n",
            "Epoch [44/50], Step [0/782], D Loss: 1.956822156906128, G Loss: 95.36287689208984\n",
            "Epoch [44/50], Step [100/782], D Loss: 2.027289390563965, G Loss: 21.9621524810791\n",
            "Epoch [44/50], Step [200/782], D Loss: 2.0395827293395996, G Loss: 21.287866592407227\n",
            "Epoch [44/50], Step [300/782], D Loss: 2.050577163696289, G Loss: 22.848323822021484\n",
            "Epoch [44/50], Step [400/782], D Loss: 2.0636844635009766, G Loss: 21.852752685546875\n",
            "Epoch [44/50], Step [500/782], D Loss: 2.116722822189331, G Loss: 21.847795486450195\n",
            "Epoch [44/50], Step [600/782], D Loss: 2.108037233352661, G Loss: 19.83953285217285\n",
            "Epoch [44/50], Step [700/782], D Loss: 2.0320088863372803, G Loss: 19.26687240600586\n",
            "Epoch [45/50], Step [0/782], D Loss: 1.8168567419052124, G Loss: 24.383073806762695\n",
            "Epoch [45/50], Step [100/782], D Loss: 1.9256948232650757, G Loss: 20.63193702697754\n",
            "Epoch [45/50], Step [200/782], D Loss: 2.020092248916626, G Loss: 18.764066696166992\n",
            "Epoch [45/50], Step [300/782], D Loss: 2.052189588546753, G Loss: 20.476701736450195\n",
            "Epoch [45/50], Step [400/782], D Loss: 2.1624879837036133, G Loss: 22.574750900268555\n",
            "Epoch [45/50], Step [500/782], D Loss: 1.9798146486282349, G Loss: 19.00971031188965\n",
            "Epoch [45/50], Step [600/782], D Loss: 2.1628286838531494, G Loss: 26.079757690429688\n",
            "Epoch [45/50], Step [700/782], D Loss: 2.058601140975952, G Loss: 20.639318466186523\n",
            "Epoch [46/50], Step [0/782], D Loss: 1.9702644348144531, G Loss: 23.4435977935791\n",
            "Epoch [46/50], Step [100/782], D Loss: 1.9300955533981323, G Loss: 20.207313537597656\n",
            "Epoch [46/50], Step [200/782], D Loss: 1.9229423999786377, G Loss: 21.584239959716797\n",
            "Epoch [46/50], Step [300/782], D Loss: 2.166592597961426, G Loss: 23.40727424621582\n",
            "Epoch [46/50], Step [400/782], D Loss: 2.0853843688964844, G Loss: 20.654325485229492\n",
            "Epoch [46/50], Step [500/782], D Loss: 1.946347951889038, G Loss: 24.697593688964844\n",
            "Epoch [46/50], Step [600/782], D Loss: 2.0418789386749268, G Loss: 24.26876449584961\n",
            "Epoch [46/50], Step [700/782], D Loss: 1.9505164623260498, G Loss: 20.741697311401367\n",
            "Epoch [47/50], Step [0/782], D Loss: 2.0055346488952637, G Loss: 22.728322982788086\n",
            "Epoch [47/50], Step [100/782], D Loss: 2.24027943611145, G Loss: 21.34254264831543\n",
            "Epoch [47/50], Step [200/782], D Loss: 2.0211853981018066, G Loss: 19.704631805419922\n",
            "Epoch [47/50], Step [300/782], D Loss: 1.881421685218811, G Loss: 18.773181915283203\n",
            "Epoch [47/50], Step [400/782], D Loss: 1.9590977430343628, G Loss: 20.530433654785156\n",
            "Epoch [47/50], Step [500/782], D Loss: 1.959837794303894, G Loss: 24.20964813232422\n",
            "Epoch [47/50], Step [600/782], D Loss: 2.047285795211792, G Loss: 18.90636444091797\n",
            "Epoch [47/50], Step [700/782], D Loss: 2.2576348781585693, G Loss: 19.589923858642578\n",
            "Epoch [48/50], Step [0/782], D Loss: 2.0230746269226074, G Loss: 27.534542083740234\n",
            "Epoch [48/50], Step [100/782], D Loss: 1.9866191148757935, G Loss: 20.876506805419922\n",
            "Epoch [48/50], Step [200/782], D Loss: 1.862684965133667, G Loss: 19.137065887451172\n",
            "Epoch [48/50], Step [300/782], D Loss: 1.8576273918151855, G Loss: 23.205413818359375\n",
            "Epoch [48/50], Step [400/782], D Loss: 2.047356605529785, G Loss: 20.612512588500977\n",
            "Epoch [48/50], Step [500/782], D Loss: 1.8682444095611572, G Loss: 20.22735023498535\n",
            "Epoch [48/50], Step [600/782], D Loss: 2.1913394927978516, G Loss: 22.71122932434082\n",
            "Epoch [48/50], Step [700/782], D Loss: 1.9749112129211426, G Loss: 21.6170597076416\n",
            "Epoch [49/50], Step [0/782], D Loss: 1.8977166414260864, G Loss: 20.853622436523438\n",
            "Epoch [49/50], Step [100/782], D Loss: 1.7486059665679932, G Loss: 19.375106811523438\n",
            "Epoch [49/50], Step [200/782], D Loss: 1.8699405193328857, G Loss: 20.68521499633789\n",
            "Epoch [49/50], Step [300/782], D Loss: 1.9275084733963013, G Loss: 19.364049911499023\n",
            "Epoch [49/50], Step [400/782], D Loss: 1.8832772970199585, G Loss: 21.47929573059082\n",
            "Epoch [49/50], Step [500/782], D Loss: 2.063084840774536, G Loss: 21.07716178894043\n",
            "Epoch [49/50], Step [600/782], D Loss: 2.198289632797241, G Loss: 19.071136474609375\n",
            "Epoch [49/50], Step [700/782], D Loss: 2.0098729133605957, G Loss: 22.723703384399414\n",
            "\n",
            "Evaluating for Class 0...\n",
            "Step [0/1500], Loss: nan\n",
            "Step [100/1500], Loss: nan\n",
            "Step [200/1500], Loss: nan\n",
            "Step [300/1500], Loss: nan\n",
            "Step [400/1500], Loss: nan\n",
            "Step [500/1500], Loss: nan\n",
            "Step [600/1500], Loss: nan\n",
            "Step [700/1500], Loss: nan\n",
            "Step [800/1500], Loss: nan\n",
            "Step [900/1500], Loss: nan\n",
            "Step [1000/1500], Loss: nan\n",
            "Step [1100/1500], Loss: nan\n",
            "Step [1200/1500], Loss: nan\n",
            "Step [1300/1500], Loss: nan\n",
            "Step [1400/1500], Loss: nan\n",
            "\n",
            "Evaluation Results for Class 0:\n",
            "Attack Accuracy: 0.00%\n",
            "KNN Distance: 1410.9981\n",
            "FID Score: 53079160.6255\n",
            "\n",
            "Evaluating for Class 1...\n",
            "Step [0/1500], Loss: nan\n",
            "Step [100/1500], Loss: nan\n",
            "Step [200/1500], Loss: nan\n",
            "Step [300/1500], Loss: nan\n",
            "Step [400/1500], Loss: nan\n",
            "Step [500/1500], Loss: nan\n",
            "Step [600/1500], Loss: nan\n",
            "Step [700/1500], Loss: nan\n",
            "Step [800/1500], Loss: nan\n",
            "Step [900/1500], Loss: nan\n",
            "Step [1000/1500], Loss: nan\n",
            "Step [1100/1500], Loss: nan\n",
            "Step [1200/1500], Loss: nan\n",
            "Step [1300/1500], Loss: nan\n",
            "Step [1400/1500], Loss: nan\n",
            "\n",
            "Evaluation Results for Class 1:\n",
            "Attack Accuracy: 0.00%\n",
            "KNN Distance: 2915.2951\n",
            "FID Score: 56926721.8874\n",
            "\n",
            "Evaluating for Class 2...\n",
            "Step [0/1500], Loss: nan\n",
            "Step [100/1500], Loss: nan\n",
            "Step [200/1500], Loss: nan\n",
            "Step [300/1500], Loss: nan\n",
            "Step [400/1500], Loss: nan\n",
            "Step [500/1500], Loss: nan\n",
            "Step [600/1500], Loss: nan\n",
            "Step [700/1500], Loss: nan\n",
            "Step [800/1500], Loss: nan\n",
            "Step [900/1500], Loss: nan\n",
            "Step [1000/1500], Loss: nan\n",
            "Step [1100/1500], Loss: nan\n",
            "Step [1200/1500], Loss: nan\n",
            "Step [1300/1500], Loss: nan\n",
            "Step [1400/1500], Loss: nan\n",
            "\n",
            "Evaluation Results for Class 2:\n",
            "Attack Accuracy: 30.00%\n",
            "KNN Distance: 1586.3298\n",
            "FID Score: 41257640.3606\n",
            "\n",
            "Evaluating for Class 3...\n",
            "Step [0/1500], Loss: nan\n",
            "Step [100/1500], Loss: nan\n",
            "Step [200/1500], Loss: nan\n",
            "Step [300/1500], Loss: nan\n",
            "Step [400/1500], Loss: nan\n",
            "Step [500/1500], Loss: nan\n",
            "Step [600/1500], Loss: nan\n",
            "Step [700/1500], Loss: nan\n",
            "Step [800/1500], Loss: nan\n",
            "Step [900/1500], Loss: nan\n",
            "Step [1000/1500], Loss: nan\n",
            "Step [1100/1500], Loss: nan\n",
            "Step [1200/1500], Loss: nan\n",
            "Step [1300/1500], Loss: nan\n",
            "Step [1400/1500], Loss: nan\n",
            "\n",
            "Evaluation Results for Class 3:\n",
            "Attack Accuracy: 3.00%\n",
            "KNN Distance: 2468.8564\n",
            "FID Score: 49151311.4779\n",
            "\n",
            "Evaluating for Class 4...\n",
            "Step [0/1500], Loss: nan\n",
            "Step [100/1500], Loss: nan\n",
            "Step [200/1500], Loss: nan\n",
            "Step [300/1500], Loss: nan\n",
            "Step [400/1500], Loss: nan\n",
            "Step [500/1500], Loss: nan\n",
            "Step [600/1500], Loss: nan\n",
            "Step [700/1500], Loss: nan\n",
            "Step [800/1500], Loss: nan\n",
            "Step [900/1500], Loss: nan\n",
            "Step [1000/1500], Loss: nan\n",
            "Step [1100/1500], Loss: nan\n",
            "Step [1200/1500], Loss: nan\n",
            "Step [1300/1500], Loss: nan\n",
            "Step [1400/1500], Loss: nan\n",
            "\n",
            "Evaluation Results for Class 4:\n",
            "Attack Accuracy: 0.00%\n",
            "KNN Distance: 1949.2503\n",
            "FID Score: 39033215.9645\n",
            "\n",
            "Evaluating for Class 5...\n",
            "Step [0/1500], Loss: nan\n",
            "Step [100/1500], Loss: nan\n",
            "Step [200/1500], Loss: nan\n",
            "Step [300/1500], Loss: nan\n",
            "Step [400/1500], Loss: nan\n",
            "Step [500/1500], Loss: nan\n",
            "Step [600/1500], Loss: nan\n",
            "Step [700/1500], Loss: nan\n",
            "Step [800/1500], Loss: nan\n",
            "Step [900/1500], Loss: nan\n",
            "Step [1000/1500], Loss: nan\n",
            "Step [1100/1500], Loss: nan\n",
            "Step [1200/1500], Loss: nan\n",
            "Step [1300/1500], Loss: nan\n",
            "Step [1400/1500], Loss: nan\n",
            "\n",
            "Evaluation Results for Class 5:\n",
            "Attack Accuracy: 0.00%\n",
            "KNN Distance: 2485.8192\n",
            "FID Score: 49225273.6268\n",
            "\n",
            "Evaluating for Class 6...\n",
            "Step [0/1500], Loss: nan\n",
            "Step [100/1500], Loss: nan\n",
            "Step [200/1500], Loss: nan\n",
            "Step [300/1500], Loss: nan\n",
            "Step [400/1500], Loss: nan\n",
            "Step [500/1500], Loss: nan\n",
            "Step [600/1500], Loss: nan\n",
            "Step [700/1500], Loss: nan\n",
            "Step [800/1500], Loss: nan\n",
            "Step [900/1500], Loss: nan\n",
            "Step [1000/1500], Loss: nan\n",
            "Step [1100/1500], Loss: nan\n",
            "Step [1200/1500], Loss: nan\n",
            "Step [1300/1500], Loss: nan\n",
            "Step [1400/1500], Loss: nan\n",
            "\n",
            "Evaluation Results for Class 6:\n",
            "Attack Accuracy: 0.00%\n",
            "KNN Distance: 1637.4173\n",
            "FID Score: 42712936.6244\n",
            "\n",
            "Evaluating for Class 7...\n",
            "Step [0/1500], Loss: nan\n",
            "Step [100/1500], Loss: nan\n",
            "Step [200/1500], Loss: nan\n",
            "Step [300/1500], Loss: nan\n",
            "Step [400/1500], Loss: nan\n",
            "Step [500/1500], Loss: nan\n",
            "Step [600/1500], Loss: nan\n",
            "Step [700/1500], Loss: nan\n",
            "Step [800/1500], Loss: nan\n",
            "Step [900/1500], Loss: nan\n",
            "Step [1000/1500], Loss: nan\n",
            "Step [1100/1500], Loss: nan\n",
            "Step [1200/1500], Loss: nan\n",
            "Step [1300/1500], Loss: nan\n",
            "Step [1400/1500], Loss: nan\n",
            "\n",
            "Evaluation Results for Class 7:\n",
            "Attack Accuracy: 0.00%\n",
            "KNN Distance: 2837.0450\n",
            "FID Score: 51186918.3480\n",
            "\n",
            "Evaluating for Class 8...\n",
            "Step [0/1500], Loss: nan\n",
            "Step [100/1500], Loss: nan\n",
            "Step [200/1500], Loss: nan\n",
            "Step [300/1500], Loss: nan\n",
            "Step [400/1500], Loss: nan\n",
            "Step [500/1500], Loss: nan\n",
            "Step [600/1500], Loss: nan\n",
            "Step [700/1500], Loss: nan\n",
            "Step [800/1500], Loss: nan\n",
            "Step [900/1500], Loss: nan\n",
            "Step [1000/1500], Loss: nan\n",
            "Step [1100/1500], Loss: nan\n",
            "Step [1200/1500], Loss: nan\n",
            "Step [1300/1500], Loss: nan\n",
            "Step [1400/1500], Loss: nan\n",
            "\n",
            "Evaluation Results for Class 8:\n",
            "Attack Accuracy: 18.00%\n",
            "KNN Distance: 1739.8345\n",
            "FID Score: 48939059.9362\n",
            "\n",
            "Evaluating for Class 9...\n",
            "Step [0/1500], Loss: nan\n",
            "Step [100/1500], Loss: nan\n",
            "Step [200/1500], Loss: nan\n",
            "Step [300/1500], Loss: nan\n",
            "Step [400/1500], Loss: nan\n",
            "Step [500/1500], Loss: nan\n",
            "Step [600/1500], Loss: nan\n",
            "Step [700/1500], Loss: nan\n",
            "Step [800/1500], Loss: nan\n",
            "Step [900/1500], Loss: nan\n",
            "Step [1000/1500], Loss: nan\n",
            "Step [1100/1500], Loss: nan\n",
            "Step [1200/1500], Loss: nan\n",
            "Step [1300/1500], Loss: nan\n",
            "Step [1400/1500], Loss: nan\n",
            "\n",
            "Evaluation Results for Class 9:\n",
            "Attack Accuracy: 3.00%\n",
            "KNN Distance: 3025.3260\n",
            "FID Score: 57580420.9779\n"
          ]
        }
      ],
      "source": [
        "def evaluate_attack(generator, evaluation_classifier, real_loader, target_label=0, num_samples=100):\n",
        "    # Calculate Attack Accuracy\n",
        "    attack_accuracy = calculate_attack_accuracy(generator, evaluation_classifier, target_label, num_samples)\n",
        "\n",
        "    # Calculate KNN Distance\n",
        "    knn_dist = calculate_knn_distance(generator, evaluation_classifier, real_loader, target_label, num_samples)\n",
        "\n",
        "    # Generate Samples for FID Calculation\n",
        "    z = torch.randn(num_samples, 100, 1, 1, device=device)\n",
        "    generated_images = generator(z)\n",
        "    fake_features = extract_features(generated_images, evaluation_classifier)\n",
        "\n",
        "    # Get Real Features\n",
        "    real_features = []\n",
        "    for images, labels in real_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        target_images = images[labels == target_label]\n",
        "        if len(target_images) > 0:\n",
        "            real_features.append(extract_features(target_images, evaluation_classifier))\n",
        "\n",
        "    real_features = np.vstack(real_features)\n",
        "    fid_score = calculate_fid(real_features, fake_features)\n",
        "\n",
        "    # Print All Metrics\n",
        "    print(f\"\\nEvaluation Results for Class {target_label}:\")\n",
        "    print(f\"Attack Accuracy: {attack_accuracy * 100:.2f}%\")\n",
        "    print(f\"KNN Distance: {knn_dist:.4f}\")\n",
        "    print(f\"FID Score: {fid_score:.4f}\")\n",
        "\n",
        "# Initialize Models\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "# CIFAR-10 Specific Classifier\n",
        "target_model = resnet18(pretrained=False, num_classes=10).to(device)\n",
        "\n",
        "# Train Inversion-Specific GAN\n",
        "train_inversion_gan(generator, discriminator, target_model, public_loader)\n",
        "\n",
        "# Loop over all 10 classes in CIFAR-10 (0 to 9)\n",
        "for target_label in range(10):\n",
        "    print(f\"\\nEvaluating for Class {target_label}...\")\n",
        "\n",
        "    # Distributional Recovery for the current class\n",
        "    recovery = DistributionalRecovery(generator, discriminator, target_model, num_classes=10, device=device)\n",
        "    recovery.update_distribution(target_label=target_label)\n",
        "\n",
        "    # Evaluate Attack Performance for the current class\n",
        "    evaluate_attack(generator, target_model, public_loader, target_label=target_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4iT1AeIwAeEX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}